{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a28949b",
   "metadata": {},
   "source": [
    "## Paper Implementation - Attention Is All You Need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda8a4ee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68349d1c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "065d59af",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e370c87",
   "metadata": {},
   "source": [
    "<img src='https://pytorch.org/tutorials/_images/transformer_architecture.jpg'>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2f7b784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fceaec",
   "metadata": {},
   "source": [
    "## Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b61b02b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_dim) -> None:\n",
    "        super(Embedding,self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embed_layer = nn.Embedding(self.vocab_size,self.embedding_dim)\n",
    "    def forward(self,x):\n",
    "        return self.embed_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c527230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14da341",
   "metadata": {},
   "source": [
    "In this step we generate positional encoding. Then we add embedding output and positional encoding.\n",
    "\n",
    "<p align=\"center\"><img height=300 src='http://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png'></p>\n",
    "\n",
    "In `Attention Is All You Need` paper auther two positional encoding function. Use sine function for even time steps and cosine function for odd time steps.\n",
    "\n",
    "<p align=\"center\"><img src='image/pe.png'></p>\n",
    "\n",
    "Here `pos` is position of the token in sentence.<br>\n",
    "`i` is the position of the dimension<br>\n",
    "$d_{model}$ is the embedding dimension<br>\n",
    "\n",
    "Hare Embedding layer output dimension and Positional Encoding layer output dimension same\n",
    "\n",
    "Reference \n",
    "- https://kazemnejad.com/blog/transformer_architecture_positional_encoding/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a382d0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,max_seq_len,d_model=512) -> None:\n",
    "        super(PositionalEncoding,self).__init__()\n",
    "        self.d_model = d_model\n",
    "        position = torch.arange(max_seq_len)#.unsqueeze(1)\n",
    "        pe = torch.zeros((max_seq_len,d_model))\n",
    "        for i in range(0,d_model,2):\n",
    "            pe[:,i] = torch.sin(position/10000**(2*i/d_model))\n",
    "            pe[:,i+1] = torch.cos(position/10000**(2*i/d_model))\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self,x):\n",
    "        return x+self.pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de027ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_len = 100\n",
    "d_model = 512\n",
    "pe_en = PositionalEncoding(max_len,d_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dd6914",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb39d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_model = 512,n_head = 8) -> None:\n",
    "        super().__init__()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed846bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 256])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 100\n",
    "d_model = 512\n",
    "position = torch.arange(max_len).unsqueeze(1)\n",
    "div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "pe = torch.zeros(max_len, 1, d_model)\n",
    "pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "pe[:, 0, 1::2] = torch.cos(position * div_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d4be66",
   "metadata": {},
   "source": [
    "https://ai.stackexchange.com/questions/41670/why-use-exponential-and-log-in-positional-encoding-of-transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85407c46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
